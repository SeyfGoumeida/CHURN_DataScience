{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py#visualization\n",
    "py.init_notebook_mode(connected=True)#visualization\n",
    "import plotly.graph_objs as go#visualization\n",
    "import plotly.tools as tls#visualization\n",
    "import plotly.figure_factory as ff#visualization\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1. Confusion matrix</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Diag: TN [0,0], TP [1,1] \n",
    "# off-Diag: FN [0,1], FP [1,0]\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true Churn - pred Churn -- TP: 7\n",
    "# true non Churn - pred non Churn -- TN: 6\n",
    "# true non Churn - pred Churn -- FP: 2\n",
    "# true Churn - pred non Churn -- FN: 8\n",
    "# [7, 8]\n",
    "# [2, 6]\n",
    "conf_matrix_disp = conf_matrix[:, ::-1]\n",
    "#conf_matrix = conf_matrix[::-1, :]\n",
    "conf_matrix_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "conf_matrix_disp_plot = go.Heatmap(z = conf_matrix_disp ,\n",
    "                        x = [\"(pred) Churn\",\"(pred) Not churn\"],\n",
    "                        y = [\"(true) Not churn\",\"(true) Churn\"],\n",
    "                        showscale  = False, colorscale = \"Picnic\",\n",
    "                        name = \"matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[conf_matrix_disp_plot],\n",
    "    layout_title_text=\"Confusion matrix\"\n",
    ")\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>2. Precision, Recall and Specificity</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision; TP/(TP+FP)\n",
    "print(\"Precision {:.4f}\".format(conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,0])))\n",
    "# recall; TP/(TP+FN)\n",
    "print(\"Recall {:.4f}\".format(conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[0,1])))\n",
    "# specificity; TN/(TN+FP)\n",
    "print(\"Specificity {:.4f}\".format(conf_matrix[1,0]/(conf_matrix[1,0]+conf_matrix[0,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>3. ROC curve</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an arbitrary data \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate two class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, \n",
    "                           n_features=20, random_state=27)\n",
    "\n",
    "# split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of two classifiers\n",
    "\n",
    "# train models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# logistic regression\n",
    "model1 = LogisticRegression()\n",
    "# knn\n",
    "model2 = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# fit model\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities\n",
    "pred_prob1 = model1.predict_proba(X_test)\n",
    "pred_prob2 = model2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# roc curve for models\n",
    "fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\n",
    "fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\n",
    "\n",
    "# roc curve for tpr = fpr \n",
    "random_probs = [0 for i in range(len(y_test))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# plot roc curves\n",
    "plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\n",
    "plt.plot(fpr2, tpr2, linestyle='--',color='green', label='KNN')\n",
    "plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "# title\n",
    "plt.title('ROC curve')\n",
    "# x label\n",
    "plt.xlabel('False Positive Rate')\n",
    "# y label\n",
    "plt.ylabel('True Positive rate')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('ROC',dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>4. AUC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# auc scores\n",
    "auc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\n",
    "auc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\n",
    "\n",
    "print(\"{:.4f}\".format(auc_score1), \"{:.4f}\".format(auc_score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "LR_trace = go.Scatter(x = fpr1,y = tpr1,\n",
    "                    name = \"Roc : \" + \"{:.4f}\".format(auc_score1),\n",
    "                    line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n",
    "KNN_trace = go.Scatter(x = fpr2,y = tpr2,\n",
    "                    name = \"Roc : \" + \"{:.4f}\".format(auc_score2),\n",
    "                    line = dict(color = ('rgb(22, 96, 0)'),width = 2))\n",
    "random_trace = go.Scatter(x = [0,1],y=[0,1],\n",
    "                    line = dict(color = ('rgb(205, 12, 24)'),\n",
    "                                width = 2,\n",
    "                    dash = 'dot'))\n",
    "\n",
    "py.iplot([LR_trace, KNN_trace, random_trace])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
